{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\anshengqiang\\anaconda3\\lib\\site-packages\\transformers\\modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2-xl and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'h.24.attn.masked_bias', 'h.25.attn.masked_bias', 'h.26.attn.masked_bias', 'h.27.attn.masked_bias', 'h.28.attn.masked_bias', 'h.29.attn.masked_bias', 'h.30.attn.masked_bias', 'h.31.attn.masked_bias', 'h.32.attn.masked_bias', 'h.33.attn.masked_bias', 'h.34.attn.masked_bias', 'h.35.attn.masked_bias', 'h.36.attn.masked_bias', 'h.37.attn.masked_bias', 'h.38.attn.masked_bias', 'h.39.attn.masked_bias', 'h.40.attn.masked_bias', 'h.41.attn.masked_bias', 'h.42.attn.masked_bias', 'h.43.attn.masked_bias', 'h.44.attn.masked_bias', 'h.45.attn.masked_bias', 'h.46.attn.masked_bias', 'h.47.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generate = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-xl\",\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I like the cat,\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.22559928894043\n",
      "Paper 1: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In\n",
      "\n",
      "\n",
      "Question 1: What approach does paper 1 propose?\n",
      "\n",
      "Answer 1: a new language representation model called BERT\n",
      "\n",
      "Paper 2: Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).\n",
      "\n",
      "Question 2: What approach does paper 2 propose?\n",
      "\n",
      "Answer 2: generative pre-training of a language model\n",
      "\n",
      "Paper 4: Advantages and disadvantages of fixation of fundamental physical constants' values for definitions of SI units are considered. The case with a new definition of the mass unit on the base of a fixed value of the Avogadro constant is studied in detail. Criteria on choosing of a optimum FPC set with fixed values for the redefinition of the SI units are suggested. The minimal optimum FPC set that is consistent with the criteria is presented. The set comprises the speed of light, the constant of the ground state hyperfine transition of the caesium-133 atom, the Avogadro constant, the mass of the carbon-12 atom and the absolute magnitude of the electron charge. Comment on the redefinition of the kelvin is also made.\n",
      "\n",
      "Question 4:  What approach does paper 4 propose?\n",
      "\n",
      "Answer 4: fixation of fundamental physical constants' values for definitions of SI units\n",
      "\n",
      "Paper 5: The problem of the definition of the mass unit on the base of a fixed value of the Avogadro constant is studied. The case of a new definition of the mass unit on the base of a fixed value of the Avogadro constant is studied. The definition of the mass unit on the base of a fixed value of the Avogadro constant is defined as the mass of the carbon-12 atom divided by the mass of the carbon-12 atom divided by the mass of the carbon-12 atom. The mass of the carbon-12 atom is defined as the mass of the carbon-12 atom divided by the mass of the carbon-12 atom divided by the mass of the carbon-12 atom. The mass of the carbon-12 atom is defined as the mass of the carbon-12 atom divided by the mass of the carbon-12 atom divided by the mass of the carbon-12 atom. The mass of the carbon-12 atom is defined as the mass of the carbon-12 atom divided by the mass of the carbon-12 atom divided by the mass of the carbon-12 atom. The mass of the carbon-12 atom is defined as the mass of the carbon-12 atom divided by the mass of the carbon-12 atom divided by the mass of the carbon-12 atom\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time0 = time.time()\n",
    "result = generate(text, max_length=1023, top_k=1, temperature=1, num_return_sequences=1, early_stopping=True)\n",
    "print(time.time()-time0)\n",
    "for item in result:\n",
    "  print(item[\"generated_text\"])\n",
    "  print()\n",
    "  print(\"=\"*50)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}